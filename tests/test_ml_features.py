"""Tests for machine learning feature selection functionality.

Real implementation testing for biological feature selection methods.
No mocking used - all tests use real computational methods and data.
"""

from __future__ import annotations

from typing import List, Tuple

import numpy as np
import pytest

from metainformant.ml.features import (
    biological_feature_ranking,
    select_features_recursive,
    select_features_stability,
    select_features_univariate,
)


class TestUnivariateFeatureSelection:
    """Test univariate statistical feature selection methods."""

    def setup_method(self):
        """Set up test data with known statistical properties."""
        np.random.seed(42)
        self.n_samples = 100
        self.n_features = 50

        # Create data with some informative and some random features
        # First 10 features are informative, rest are random
        X_informative = np.random.randn(self.n_samples, 10)
        X_random = np.random.randn(self.n_samples, 40)

        # Create binary classification target correlated with informative features
        y_continuous = np.sum(X_informative[:, :5], axis=1) + np.random.randn(self.n_samples) * 0.1
        self.y_binary = (y_continuous > np.median(y_continuous)).astype(int)

        # Combine informative and random features
        self.X = np.hstack([X_informative, X_random])

        # Multi-class target
        self.y_multiclass = np.random.randint(0, 3, self.n_samples)
        # Make first few features informative for multi-class
        for i in range(3):
            class_mask = self.y_multiclass == i
            self.X[class_mask, i] += 2.0  # Add class-specific signal

    def test_f_score_feature_selection(self):
        """Test F-score based feature selection."""
        X_selected, selected_indices = select_features_univariate(
            self.X, self.y_binary, method="f_score", k=15, p_threshold=0.05
        )

        # Check output dimensions
        assert X_selected.shape == (self.n_samples, 15)
        assert len(selected_indices) == 15

        # Selected features should include some informative ones
        # (first 10 features are informative)
        informative_selected = sum(1 for idx in selected_indices if idx < 10)
        assert informative_selected > 5  # Should select most informative features

        # Check that indices are valid
        assert all(0 <= idx < self.n_features for idx in selected_indices)
        assert len(set(selected_indices)) == 15  # No duplicates

    def test_chi2_feature_selection(self):
        """Test Chi-squared based feature selection."""
        # Make features non-negative for chi2 test
        X_positive = np.abs(self.X)

        X_selected, selected_indices = select_features_univariate(X_positive, self.y_binary, method="chi2", k=20)

        assert X_selected.shape == (self.n_samples, 20)
        assert len(selected_indices) == 20
        assert np.all(X_selected >= 0)  # Should remain non-negative

    def test_mutual_info_feature_selection(self):
        """Test mutual information based feature selection."""
        X_selected, selected_indices = select_features_univariate(self.X, self.y_multiclass, method="mutual_info", k=12)

        assert X_selected.shape == (self.n_samples, 12)
        assert len(selected_indices) == 12

        # For multi-class, features 0, 1, 2 should be highly selected
        # (they have class-specific signals)
        top_3_selected = sum(1 for idx in selected_indices if idx < 3)
        assert top_3_selected >= 2  # Should select most of the informative features

    def test_invalid_method_error(self):
        """Test error handling for invalid method."""
        with pytest.raises(ValueError, match="Unknown method"):
            select_features_univariate(self.X, self.y_binary, method="invalid_method")

    def test_dimension_mismatch_error(self):
        """Test error handling for dimension mismatch."""
        # Wrong number of samples
        y_wrong = np.random.randint(0, 2, self.n_samples + 5)

        with pytest.raises(ValueError, match="y length must match X samples"):
            select_features_univariate(self.X, y_wrong, method="f_score")

        # Wrong X dimensions
        X_wrong = np.random.randn(self.n_samples, 10, 5)  # 3D instead of 2D

        with pytest.raises(ValueError, match="X must be 2D array"):
            select_features_univariate(X_wrong, self.y_binary, method="f_score")

    def test_k_larger_than_features(self):
        """Test behavior when k is larger than number of features."""
        X_small = self.X[:, :20]  # Only 20 features

        X_selected, selected_indices = select_features_univariate(
            X_small, self.y_binary, method="f_score", k=30  # Ask for 30 > 20
        )

        # Should return all 20 features
        assert X_selected.shape == (self.n_samples, 20)
        assert len(selected_indices) == 20

    def test_multiclass_classification(self):
        """Test feature selection with multi-class targets."""
        X_selected, selected_indices = select_features_univariate(self.X, self.y_multiclass, method="f_score", k=10)

        assert X_selected.shape == (self.n_samples, 10)

        # Features 0, 1, 2 have class-specific signals, so should be selected
        top_features_selected = sum(1 for idx in selected_indices if idx < 3)
        assert top_features_selected >= 2


class TestRecursiveFeatureElimination:
    """Test recursive feature elimination methods."""

    def setup_method(self):
        """Set up test data for RFE."""
        np.random.seed(123)
        self.n_samples = 80
        self.n_features = 30

        # Create data with linear relationships
        true_coefficients = np.zeros(self.n_features)
        true_coefficients[:8] = np.array([2, -1.5, 1, -0.8, 1.2, -2, 0.5, -1])  # First 8 are important

        self.X = np.random.randn(self.n_samples, self.n_features)
        noise = np.random.randn(self.n_samples) * 0.1
        self.y_continuous = self.X @ true_coefficients + noise
        self.y_binary = (self.y_continuous > np.median(self.y_continuous)).astype(int)

    def test_recursive_feature_elimination_basic(self):
        """Test basic RFE functionality."""
        X_selected, selected_indices = select_features_recursive(
            self.X, self.y_binary, estimator_type="random_forest", n_features=10, step=0.2, random_state=42
        )

        # Check output dimensions
        assert X_selected.shape == (self.n_samples, 10)
        assert len(selected_indices) == 10

        # Should preferentially select important features (first 8)
        important_selected = sum(1 for idx in selected_indices if idx < 8)
        assert important_selected >= 6  # Should get most important features

        # Check indices are valid and unique
        assert all(0 <= idx < self.n_features for idx in selected_indices)
        assert len(set(selected_indices)) == 10

    def test_rfe_different_estimators(self):
        """Test RFE with different base estimators."""
        estimators = ["random_forest", "linear", "svm"]

        for estimator in estimators:
            X_selected, selected_indices = select_features_recursive(
                self.X, self.y_binary, estimator_type=estimator, n_features=8, random_state=42
            )

            assert X_selected.shape == (self.n_samples, 8)
            assert len(selected_indices) == 8

    def test_rfe_step_size(self):
        """Test RFE with different step sizes."""
        # Large step (remove many features at once)
        X_large_step, indices_large = select_features_recursive(
            self.X, self.y_binary, n_features=10, step=0.5, random_state=42  # Remove 50% each step
        )

        # Small step (remove few features at once)
        X_small_step, indices_small = select_features_recursive(
            self.X, self.y_binary, n_features=10, step=0.1, random_state=42  # Remove 10% each step
        )

        # Both should return same number of features
        assert X_large_step.shape == X_small_step.shape == (self.n_samples, 10)

        # Results might be different due to different elimination paths
        # But should be reasonable selections
        assert len(set(indices_large)) == 10
        assert len(set(indices_small)) == 10

    def test_rfe_reproducibility(self):
        """Test RFE reproducibility with fixed random state."""
        X_selected1, indices1 = select_features_recursive(self.X, self.y_binary, n_features=12, random_state=99)

        X_selected2, indices2 = select_features_recursive(self.X, self.y_binary, n_features=12, random_state=99)

        # Results should be identical
        np.testing.assert_array_equal(X_selected1, X_selected2)
        assert indices1 == indices2

    def test_rfe_invalid_estimator(self):
        """Test error handling for invalid estimator."""
        with pytest.raises(ValueError, match="Unknown importance method"):
            select_features_recursive(self.X, self.y_binary, estimator_type="invalid_estimator")


class TestStabilityFeatureSelection:
    """Test stability-based feature selection."""

    def setup_method(self):
        """Set up test data for stability selection."""
        np.random.seed(456)
        self.n_samples = 60
        self.n_features = 25

        # Create data where first few features are consistently important
        self.X = np.random.randn(self.n_samples, self.n_features)

        # Make first 5 features always correlated with target
        true_signal = np.sum(self.X[:, :5], axis=1)
        noise = np.random.randn(self.n_samples) * 0.3
        y_continuous = true_signal + noise
        self.y = (y_continuous > np.median(y_continuous)).astype(int)

    def test_stability_selection_basic(self):
        """Test basic stability selection functionality."""
        X_selected, selected_indices = select_features_stability(
            self.X,
            self.y,
            method="random_forest",
            n_bootstrap=50,
            subsample_ratio=0.8,
            stability_threshold=0.4,
            random_state=789,
        )

        # Check output dimensions
        assert X_selected.shape[0] == self.n_samples
        assert X_selected.shape[1] == len(selected_indices)
        assert len(selected_indices) >= 1  # Should select at least some features

        # Should preferentially select stable important features (first 5)
        important_selected = sum(1 for idx in selected_indices if idx < 5)
        total_selected = len(selected_indices)

        # At least half of selected features should be from important ones
        if total_selected > 0:
            assert important_selected / total_selected >= 0.3

    def test_stability_selection_different_methods(self):
        """Test stability selection with different base methods."""
        methods = ["random_forest", "univariate"]

        for method in methods:
            X_selected, selected_indices = select_features_stability(
                self.X,
                self.y,
                method=method,
                n_bootstrap=20,  # Fewer iterations for speed
                stability_threshold=0.3,
                random_state=123,
            )

            assert X_selected.shape[0] == self.n_samples
            assert len(selected_indices) >= 1

    def test_stability_threshold_effect(self):
        """Test effect of stability threshold on feature selection."""
        # Low threshold (should select more features)
        X_low, indices_low = select_features_stability(
            self.X, self.y, stability_threshold=0.2, n_bootstrap=30, random_state=456
        )

        # High threshold (should select fewer features)
        X_high, indices_high = select_features_stability(
            self.X, self.y, stability_threshold=0.7, n_bootstrap=30, random_state=456
        )

        # Low threshold should generally select more features
        assert len(indices_low) >= len(indices_high)

    def test_stability_reproducibility(self):
        """Test reproducibility of stability selection."""
        X_selected1, indices1 = select_features_stability(self.X, self.y, random_state=111, n_bootstrap=25)

        X_selected2, indices2 = select_features_stability(self.X, self.y, random_state=111, n_bootstrap=25)

        # Results should be identical
        np.testing.assert_array_equal(X_selected1, X_selected2)
        assert indices1 == indices2

    def test_stability_no_features_warning(self):
        """Test warning when no features meet stability threshold."""
        # Very high threshold that no features can meet
        with pytest.warns(UserWarning, match="No features meet stability threshold"):
            X_selected, selected_indices = select_features_stability(
                self.X, self.y, stability_threshold=0.99, n_bootstrap=10, random_state=789  # Impossibly high
            )

        # Should still return some features (top ones)
        assert len(selected_indices) > 0
        assert X_selected.shape[1] == len(selected_indices)


class TestBiologicalFeatureRanking:
    """Test biological-aware feature ranking methods."""

    def setup_method(self):
        """Set up test data with feature names and biological weights."""
        np.random.seed(321)
        self.n_samples = 50
        self.n_features = 20

        # Create feature names (genes)
        self.feature_names = [f"GENE_{i}" for i in range(self.n_features)]

        # Create data where certain "genes" are more important
        self.X = np.random.randn(self.n_samples, self.n_features)

        # Make first few features statistically important
        important_indices = [0, 1, 2, 5, 7]
        for idx in important_indices:
            self.X[:, idx] += np.random.randn(self.n_samples) * 0.5

        # Create target correlated with important features
        y_signal = np.sum(self.X[:, important_indices], axis=1)
        self.y = (y_signal > np.median(y_signal)).astype(int)

        # Create biological weights (prior knowledge)
        self.gene_weights = {}
        for i, name in enumerate(self.feature_names):
            if i in [1, 3, 5, 8]:  # Some genes have high biological relevance
                self.gene_weights[name] = 2.0
            elif i in [2, 6, 9, 10]:  # Some have medium relevance
                self.gene_weights[name] = 1.5
            else:
                self.gene_weights[name] = 1.0

    def test_biological_ranking_statistical_only(self):
        """Test ranking using only statistical criteria."""
        ranked_features = biological_feature_ranking(
            self.X, self.y, feature_names=self.feature_names, method="statistical"
        )

        # Check output format
        assert len(ranked_features) == self.n_features
        for idx, name, score in ranked_features:
            assert 0 <= idx < self.n_features
            assert name in self.feature_names
            assert isinstance(score, float)
            assert score >= 0

        # Should be sorted by score (descending)
        scores = [score for _, _, score in ranked_features]
        assert scores == sorted(scores, reverse=True)

        # Statistically important features should rank highly
        top_5_indices = [idx for idx, _, _ in ranked_features[:5]]
        important_in_top = sum(1 for idx in top_5_indices if idx in [0, 1, 2, 5, 7])
        assert important_in_top >= 3  # Most important features should be in top 5

    def test_biological_ranking_biological_only(self):
        """Test ranking using only biological weights."""
        ranked_features = biological_feature_ranking(
            self.X, self.y, feature_names=self.feature_names, method="biological", gene_weights=self.gene_weights
        )

        assert len(ranked_features) == self.n_features

        # Features with high biological weights should rank highly
        top_5_names = [name for _, name, _ in ranked_features[:5]]
        high_weight_genes = [name for name, weight in self.gene_weights.items() if weight >= 1.5]

        # At least some high-weight genes should be in top 5
        overlap = len(set(top_5_names).intersection(set(high_weight_genes)))
        assert overlap >= 2

    def test_biological_ranking_combined(self):
        """Test combined statistical + biological ranking."""
        ranked_features = biological_feature_ranking(
            self.X, self.y, feature_names=self.feature_names, method="combined", gene_weights=self.gene_weights
        )

        assert len(ranked_features) == self.n_features

        # Combined method should balance both criteria
        # Features that are both statistically and biologically important should rank highest
        top_3_indices = [idx for idx, _, _ in ranked_features[:3]]

        # Feature 1 and 5 are both statistically important AND have high biological weights
        combined_important = [idx for idx in [1, 5] if idx in top_3_indices]
        assert len(combined_important) >= 1

    def test_biological_ranking_no_weights(self):
        """Test ranking when no biological weights provided."""
        ranked_features = biological_feature_ranking(
            self.X,
            self.y,
            feature_names=self.feature_names,
            method="combined",  # Should default to equal biological weights
        )

        assert len(ranked_features) == self.n_features

        # Without biological weights, should behave like statistical ranking
        stat_only = biological_feature_ranking(self.X, self.y, feature_names=self.feature_names, method="statistical")

        # Top features should be similar (not necessarily identical due to combination)
        top_5_combined = [idx for idx, _, _ in ranked_features[:5]]
        top_5_stat = [idx for idx, _, _ in stat_only[:5]]

        overlap = len(set(top_5_combined).intersection(set(top_5_stat)))
        assert overlap >= 3  # Should have significant overlap

    def test_biological_ranking_default_feature_names(self):
        """Test ranking with default feature names."""
        ranked_features = biological_feature_ranking(
            self.X,
            self.y,
            method="statistical",
            # No feature_names provided
        )

        assert len(ranked_features) == self.n_features

        # Should generate default names
        for idx, name, score in ranked_features:
            assert name.startswith("feature_")
            assert isinstance(score, float)

    def test_biological_ranking_invalid_method(self):
        """Test error handling for invalid ranking method."""
        with pytest.raises(ValueError, match="Unknown method"):
            biological_feature_ranking(self.X, self.y, feature_names=self.feature_names, method="invalid_method")

    def test_biological_ranking_name_mismatch(self):
        """Test error handling when feature names don't match X columns."""
        wrong_names = ["GENE_A", "GENE_B"]  # Only 2 names for 20 features

        with pytest.raises(ValueError, match="feature_names length must match X columns"):
            biological_feature_ranking(self.X, self.y, feature_names=wrong_names, method="statistical")


class TestFeatureSelectionIntegration:
    """Integration tests combining multiple feature selection methods."""

    def setup_method(self):
        """Set up comprehensive test data."""
        np.random.seed(999)
        self.n_samples = 100
        self.n_features = 40

        # Create complex data with multiple types of relationships
        self.X = np.random.randn(self.n_samples, self.n_features)

        # Linear relationships (features 0-4)
        linear_coeffs = [2, -1.5, 1, -0.8, 1.2]
        linear_signal = sum(coeff * self.X[:, i] for i, coeff in enumerate(linear_coeffs))

        # Interaction effects (features 5, 6)
        interaction_signal = self.X[:, 5] * self.X[:, 6] * 0.5

        # Non-linear relationships (feature 7)
        nonlinear_signal = np.sin(self.X[:, 7]) * 0.3

        # Combine signals
        total_signal = linear_signal + interaction_signal + nonlinear_signal
        noise = np.random.randn(self.n_samples) * 0.2

        y_continuous = total_signal + noise
        self.y = (y_continuous > np.median(y_continuous)).astype(int)

        self.feature_names = [f"FEATURE_{i}" for i in range(self.n_features)]

    def test_feature_selection_consistency(self):
        """Test consistency across different feature selection methods."""
        # Apply different methods
        X_univ, indices_univ = select_features_univariate(self.X, self.y, method="f_score", k=15)

        X_rfe, indices_rfe = select_features_recursive(self.X, self.y, n_features=15, random_state=42)

        X_stab, indices_stab = select_features_stability(
            self.X, self.y, n_bootstrap=30, stability_threshold=0.3, random_state=42
        )

        # All methods should select reasonable number of features
        assert len(indices_univ) == 15
        assert len(indices_rfe) == 15
        assert len(indices_stab) >= 5  # Stability might select fewer

        # There should be some overlap between methods for truly important features
        univ_set = set(indices_univ)
        rfe_set = set(indices_rfe)
        stab_set = set(indices_stab)

        # Important features (0-7) should be commonly selected
        important_features = set(range(8))

        univ_important = len(univ_set.intersection(important_features))
        rfe_important = len(rfe_set.intersection(important_features))
        stab_important = len(stab_set.intersection(important_features))

        # Each method should identify some important features
        assert univ_important >= 4
        assert rfe_important >= 3
        assert stab_important >= 2

    def test_feature_selection_pipeline(self):
        """Test complete feature selection pipeline."""
        # Step 1: Initial univariate filtering
        X_filtered, initial_indices = select_features_univariate(self.X, self.y, method="f_score", k=25)

        # Step 2: Stability selection on filtered features
        X_stable, stable_relative_indices = select_features_stability(
            X_filtered, self.y, n_bootstrap=20, stability_threshold=0.4, random_state=123
        )

        # Convert relative indices back to original feature space
        stable_original_indices = [initial_indices[i] for i in stable_relative_indices]

        # Step 3: Biological ranking of stable features
        stable_feature_names = [self.feature_names[i] for i in stable_original_indices]

        ranked_features = biological_feature_ranking(
            X_stable, self.y, feature_names=stable_feature_names, method="statistical"
        )

        # Verify pipeline integrity
        assert X_stable.shape[0] == self.n_samples
        assert X_stable.shape[1] == len(stable_original_indices)
        assert len(ranked_features) == len(stable_original_indices)

        # Top-ranked features should include some truly important ones
        top_5_original_indices = [stable_original_indices[idx] for idx, _, _ in ranked_features[:5]]
        important_in_top = sum(1 for idx in top_5_original_indices if idx < 8)
        assert important_in_top >= 2


class TestEdgeCases:
    """Test edge cases and error conditions for feature selection."""

    def test_single_feature(self):
        """Test feature selection with single feature."""
        X_single = np.random.randn(50, 1)
        y = np.random.randint(0, 2, 50)

        # Should handle single feature gracefully
        X_selected, indices = select_features_univariate(X_single, y, k=5)  # Ask for more than available

        assert X_selected.shape == (50, 1)
        assert indices == [0]

    def test_binary_features(self):
        """Test feature selection with binary features."""
        # Create binary feature matrix
        X_binary = np.random.randint(0, 2, (80, 30))
        y = np.random.randint(0, 2, 80)

        # Should handle binary features without error
        X_selected, indices = select_features_univariate(X_binary, y, method="chi2", k=10)

        assert X_selected.shape == (80, 10)
        assert len(indices) == 10
        assert np.all(np.isin(X_selected, [0, 1]))  # Should remain binary

    def test_constant_features(self):
        """Test handling of constant (zero-variance) features."""
        X = np.random.randn(50, 10)
        X[:, 5] = 1.0  # Constant feature
        X[:, 8] = 0.0  # Another constant feature

        y = np.random.randint(0, 2, 50)

        # Should handle constant features gracefully
        X_selected, indices = select_features_univariate(X, y, method="f_score", k=8)

        assert X_selected.shape == (50, 8)
        # Constant features might or might not be selected (implementation dependent)
        # Main thing is it shouldn't crash

    def test_perfect_separation(self):
        """Test feature selection when one feature perfectly separates classes."""
        X = np.random.randn(60, 15)
        y = np.array([0] * 30 + [1] * 30)

        # Create perfectly separating feature
        X[:30, 0] = -10  # Class 0 gets very negative values
        X[30:, 0] = 10  # Class 1 gets very positive values

        X_selected, indices = select_features_univariate(X, y, method="f_score", k=5)

        assert X_selected.shape == (60, 5)
        # Perfect separator should definitely be selected
        assert 0 in indices

    def test_all_same_class(self):
        """Test feature selection when all samples have same class."""
        X = np.random.randn(40, 20)
        y = np.ones(40)  # All same class

        # Should handle gracefully (though results may not be meaningful)
        try:
            X_selected, indices = select_features_univariate(X, y, method="f_score", k=10)
            # If it doesn't crash, that's good
            assert X_selected.shape == (40, 10)
        except ValueError:
            # Some methods might legitimately fail with single class
            pass

    def test_more_features_than_samples(self):
        """Test high-dimensional case (p >> n)."""
        n_samples = 20
        n_features = 100

        X = np.random.randn(n_samples, n_features)
        y = np.random.randint(0, 2, n_samples)

        # Should handle high-dimensional data
        X_selected, indices = select_features_univariate(X, y, method="f_score", k=15)

        assert X_selected.shape == (n_samples, 15)
        assert len(indices) == 15
